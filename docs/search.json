[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AWS x NFL Big Data Bowl",
    "section": "",
    "text": "1 Introduction\nFor our final project, we have decided to enter the AWS x NFL Annual Big Data Bowl – a competition hosted on Kaggle where participants are challenged to find meaningful insights from week-to-week game data within the 2023 NFL season, creating and delivering a compelling story within the data. The competition has 2 tracks, including a University Track – the one we entered into, tasked with analyzing player movement, creating player or team metrics for either the offensive or defensive units, and/or evaluating other passing play topics.\nAs a part of this competition, we will be creating models and various types of visualizations of both model outputs, as well as the raw data itself.\nWe chose to work on this topic because we both are passionate fans of the Philadelphia Eagles and Detriot Lions, and in particular, enjoy the advanced analytic insights that AWS provides to viewers during football games. As avid fans, developing and contributing something used in practice by professional teams would be a massive accomplishment, and we are eager to put our combined domain knowledge and technical backgrounds together to deliver something that is both unique and insightful.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Description\n2.1 Technical Description:\nAll of the data for this project is directly provided by AWS/the NFL itself, via the Kaggle competition. However, the data is unseen by the general public and does not have any published work on it already. The data will be static for the duration of the project, with no updates.\nThe data can be broken down into 3 main categories:\nSupplementary\nThis is a single csv (41 columns by 18009 rows) that provides high-level contextual data about each play from the 2023 NFL season. Every row is identified by a unique play_id, coming from each unique game within a given week 1-18. This table provides info such as pre-snap offensive formation, play outcome, pre-snap score, time of game the play happened at, as well as other relevant info. Input\nThis is broken down into 18 csv files (23 columns by ~30k rows each) – one for each week of the season. Within each file, we have groups of rows for every player that participated in a game that week. For every player, there are multiple rows for every play they participated in, corresponding to the number of frames within that play. Output\nThis is also broken down into 18 csv files (6 columns by ~30k rows each), with each file again representing one week of the season. The data in this file contains the play ID, the player ID, and their final position on the field at the time the play ends.\nCode\nin_week1 &lt;- read.csv(\"data/train/input_2023_w01.csv\")\nin_week2 &lt;- read.csv(\"data/train/input_2023_w02.csv\")\nin_week3 &lt;- read.csv(\"data/train/input_2023_w03.csv\")\nin_week4 &lt;- read.csv(\"data/train/input_2023_w04.csv\")\nin_week5 &lt;- read.csv(\"data/train/input_2023_w05.csv\")\nin_week6 &lt;- read.csv(\"data/train/input_2023_w06.csv\")\nin_week7 &lt;- read.csv(\"data/train/input_2023_w07.csv\")\nin_week8 &lt;- read.csv(\"data/train/input_2023_w08.csv\")\nin_week9 &lt;- read.csv(\"data/train/input_2023_w09.csv\")\nin_week10 &lt;- read.csv(\"data/train/input_2023_w10.csv\")\nin_week11 &lt;- read.csv(\"data/train/input_2023_w11.csv\")\nin_week12 &lt;- read.csv(\"data/train/input_2023_w12.csv\")\nin_week13 &lt;- read.csv(\"data/train/input_2023_w13.csv\")\nin_week14 &lt;- read.csv(\"data/train/input_2023_w14.csv\")\nin_week15 &lt;- read.csv(\"data/train/input_2023_w15.csv\")\nin_week16 &lt;- read.csv(\"data/train/input_2023_w16.csv\")\nin_week17 &lt;- read.csv(\"data/train/input_2023_w17.csv\")\nin_week18 &lt;- read.csv(\"data/train/input_2023_w18.csv\")\n\nout_week1 &lt;- read.csv(\"data/train/output_2023_w01.csv\")\nout_week2 &lt;- read.csv(\"data/train/output_2023_w02.csv\")\nout_week3 &lt;- read.csv(\"data/train/output_2023_w03.csv\")\nout_week4 &lt;- read.csv(\"data/train/output_2023_w04.csv\")\nout_week5 &lt;- read.csv(\"data/train/output_2023_w05.csv\")\nout_week6 &lt;- read.csv(\"data/train/output_2023_w06.csv\")\nout_week7 &lt;- read.csv(\"data/train/output_2023_w07.csv\")\nout_week8 &lt;- read.csv(\"data/train/output_2023_w08.csv\")\nout_week9 &lt;- read.csv(\"data/train/output_2023_w09.csv\")\nout_week10 &lt;- read.csv(\"data/train/output_2023_w10.csv\")\nout_week11 &lt;- read.csv(\"data/train/output_2023_w11.csv\")\nout_week12 &lt;- read.csv(\"data/train/output_2023_w12.csv\")\nout_week13 &lt;- read.csv(\"data/train/output_2023_w13.csv\")\nout_week14 &lt;- read.csv(\"data/train/output_2023_w14.csv\")\nout_week15 &lt;- read.csv(\"data/train/output_2023_w15.csv\")\nout_week16 &lt;- read.csv(\"data/train/output_2023_w16.csv\")\nout_week17 &lt;- read.csv(\"data/train/output_2023_w17.csv\")\nout_week18 &lt;- read.csv(\"data/train/output_2023_w18.csv\")\n\nsupplementary &lt;- read.csv(\"data/supplementary_data.csv\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis\n\n\nCode\nall_dfs &lt;- list(file1 = in_week1, \n                file2 = in_week2, \n                file3 = in_week3, \n                file4 = in_week4, \n                file5 = in_week5, \n                file6 = in_week6, \n                file7 = in_week7, \n                file8 = in_week8, \n                file9 = in_week9, \n                file10 = in_week10, \n                file11 = in_week11, \n                file12 = in_week12, \n                file13 = in_week13, \n                file14 = in_week14, \n                file15 = in_week15, \n                file16 = in_week16, \n                file17 = in_week17, \n                file18 = in_week18, \n                file19 = out_week1, \n                file20 = out_week2, \n                file21 = out_week3, \n                file22 = out_week4, \n                file22 = out_week5, \n                file24 = out_week6, \n                file25 = out_week7, \n                file26 = out_week8, \n                file27 = out_week9, \n                file28 = out_week10, \n                file29 = out_week11, \n                file30 = out_week12, \n                file31 = out_week13, \n                file32 = out_week14, \n                file33 = out_week15, \n                file34 = out_week16, \n                file35 = out_week17, \n                file36 = out_week18, \n                file37 = supplementary\n                )\n\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nna_summary &lt;- map_dfr(names(all_dfs), function(name) {\n  df &lt;- all_dfs[[name]]\n  na_counts &lt;- colSums(is.na(df))\n  total_na &lt;- sum(na_counts)\n  \n  tibble(\n    dataframe = name,\n    total_na = total_na,\n    columns_with_na = paste(names(na_counts[na_counts &gt; 0]), collapse = \", \")\n  )\n}) %&gt;% \n  filter(total_na &gt; 0)\n\nna_summary\n\n\n# A tibble: 1 × 3\n  dataframe total_na columns_with_na                                            \n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;                                                      \n1 file37       17837 yardline_side, route_of_targeted_receiver, play_action, dr…\n\n\nCode\n## All N/A values come from the supplementary data set -- most come the \"penalty_yards\" column\nsupp_na_counts &lt;- colSums(is.na(supplementary))\nno_pen_na &lt;- is.na(supplementary['penalty_yards'])\nsum(no_pen_na)\n\n\n[1] 17549\n\n\nCode\nsum(supp_na_counts)\n\n\n[1] 17837\n\n\nCode\n# Show N/A values by column\ncols_with_na &lt;- function(df) {\n  na_counts &lt;- colSums(is.na(df))\n  na_counts[na_counts &gt; 0]\n}\ncols_with_na(supplementary)\n\n\n             yardline_side route_of_targeted_receiver \n                       251                          4 \n               play_action              dropback_type \n                         1                          1 \n         dropback_distance         pass_location_type \n                         1                         20 \n    team_coverage_man_zone         team_coverage_type \n                         5                          5 \n             penalty_yards \n                     17549",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  }
]